{"basics":{"name":"Aditya Tb","label":"Machine Learning Research Engineer | Audio Deepfake Detection","picture":null,"email":"aditya.tirumala@mail.mcgill.ca","website":"https://adityatb.github.io/","summary":"ML Research Engineer in Speech/Audio/Music with a background in Audio Engineering and Music Technology. Expertise in Differentiable Signal Processing, Audio Watermarking and Deepfake Detection. Currently researching countermeasure systems to assist in ethical use of generative AI systems. Currently based in Canada.\n","location":{"city":"Ottawa","countryCode":"CA","region":"Ontario"},"profiles":[{"network":"GitHub","username":"adityatb","url":"https://github.com/adityatb"},{"network":"LinkedIn","username":"in/adityatb","url":"https://www.linkedin.com/in/adityatb/"}]},"work":[{"company":"Resemble AI","position":"Machine Learning Research Engineer","website":"https://resemble.ai","startDate":"2020-04-06","endDate":"","summary":"Resemble AI is a voice synthesis platform that allows users to create expressive, realistic, and human-like voices.  I am currently leading the research and development of the deepfake detection system for audio.\n","highlights":["Implemented state of the art accuracy on real‑world classification of speech+audio. EER < 6% on Franhoufer's In the Wild dataset.","Improving and maintaining changes to production via CI/CD and replicate AI.","Deep Learning R&D: Neural Upsampling, Audio Watermarking (neural/non‑neural), Audio Deepfake Detection"]},{"company":"McGill University","position":"Teaching Assistant","website":"https://mcgill.ca","startDate":"2017-09-15","endDate":"2017-12-22","summary":"McGill University's Department of Music Technology teaches the MUMT 305 course on MIDI synthesis using Max/MSP and C++.\n","highlights":["Assisting course instructors and students with course material and grading assignments","Technical Skills: Max/MSP, C++, Arduino IDE"]},{"company":"CIRMMT","position":"Research Assistant","website":"https://mcgill.ca","startDate":"2017-05-15","endDate":"2017-08-22","summary":"The Centre for Interdisciplinary Research in Music Media and Technology (CIRMMT) is a multi-disciplinary research group that focuses on the\n  scientific study of music, sound, and technology. Assisted in verifying an AI based non-intrusive pose-estimation system \nwith data from a Qualisys Motion Capture system.\n","highlights":["Assisted a research team of 5 in capturing and post‑processing motion capture data for a CIRMMT Industry Partner.","Motion Capture and Post-Processing with Qualisys MoCap"]},{"company":"Sample Culture","position":"Founder, Sound Designer","startDate":"2014-08-15","endDate":"2016-08-19","summary":"Sample Culture was a location sound recording, sound design and audio production house that catered to the needs of advertising agencies, independent filmmakers and\n  musicians in Hyderabad, India.\n","highlights":["Designed and constructed a soundproof recording studio for Sample Culture, with a flat response b/w 20Hz-20kHz A-weighted.","Award winning compositions, location sound and sound design for independent films and advertising agencies."]}],"education":[{"institution":"McGill University","area":"Music Technology","studyType":"Masters","startDate":"2016-09-08","endDate":"2019-10-20","courses":["MUMT 605 - Digital Sound Synthesis and Audio Processing","MUMT 618 - Computational Modeling of Musical Acoustic Systems","MUMT 621 - Music Information Acquisition, Preservation, and Retrieval","MUMT 619 - Input Devices for Musical Expression","MUMT 620 - Gestural Control of Sound Synthesis"]},{"institution":"School of Audio Engineering","area":"Audio Engineering","studyType":"Bachelor","startDate":"2010-05-08","endDate":"2013-02-20","courses":["Procedural Audio Design with Max/MSP, Pure Data","Thesis: Procedural Footstep synthesis for video games"]},{"institution":"Jawaharlal Nehru Technological University (JNTU)","area":"Electronics and Communication Engineering","studyType":"Bachelor","startDate":"2005-09-05","endDate":"2009-05-05"}],"publications":[{"name":"Defining a vibrotactile toolkit for digital musical instruments: characterizing voice coil actuators, effects of loading, and equalization of the frequency response","publisher":"Journal on Multimodal User Interfaces 14, Springer","releaseDate":"2020-07-01","website":"https://link.springer.com/article/10.1007/s12193-020-00340-0","summary":"Given the extreme requirements of musical performances, there is a need for solutions allowing for independent control of frequency and amplitude over a wide frequency bandwidth (40–1000 Hz) and low harmonic distortion, so that flexible and high-quality vibrotactile feedback can be displayed. In this paper, we evaluate cost-effective and portable solutions that meet these requirements."},{"name":"Autoregressive parameter estimation for equalizing vibrotactile systems","publisher":"International Workshop on Haptic and Audio Interaction Design-HAID2019","releaseDate":"2019-03-10","website":"https://hal.science/hal-02013514v2/document","summary":"For advanced musical use, it is essential to perform the characterization of the amplifiers and actuators involved, as well as the equalization of their overall frequency response characteristics, a step typically implemented with the help of manually configured parametric equalizers. This paper proposes an autoregressive method that automatically estimates minimum-phase filter parameters, which by design, remain stable upon inversion."}],"skills":[{"name":"Machine Learning","level":"Master","keywords":["PyTorch","Scikit-learn","Python","Graph Neural Networks","Differentiable Signal Processing"]},{"name":"Audio Signal Processing","level":"Master","keywords":["Time-Frequency Analysis","FFT/DFT","DWT","Polyphase Filterbanks"]},{"name":"Backend Development","level":"Intermediate","keywords":["Docker","Flask","Git","FastAPI","Django"]}],"interests":[{"name":"Real-Time Differential Signal Processing","keywords":["Tone Transfer","Automatic Mixing"]},{"name":"Audio Deepfake Detection","keywords":["One Class Learning","Ensemble Learning","Spectro-Temporal Graphs"]}]}
